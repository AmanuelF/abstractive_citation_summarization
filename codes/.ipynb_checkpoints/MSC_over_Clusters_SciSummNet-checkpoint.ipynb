{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6291,
     "status": "ok",
     "timestamp": 1636992727632,
     "user": {
      "displayName": "Amanuel Alambo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1kA5oHs9X8VTfaoRu8wK35Yj33QD7CJkOZ6QBgQ=s64",
      "userId": "11985467428571358535"
     },
     "user_tz": 300
    },
    "id": "zPt3Nfp8A13M",
    "outputId": "3b7a33f4-d810-4cf3-9dfe-65ba8a5c5208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Collecting en_core_web_sm==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 4.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run in python console\n",
    "import nltk; nltk.download('stopwords')\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Run in terminal or command prompt\n",
    "!python3 -m spacy download en\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiYXMDiqCpzw"
   },
   "source": [
    "## https://github.com/boudinfl/takahe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrkyP7C8X94g"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10374,
     "status": "ok",
     "timestamp": 1636992741570,
     "user": {
      "displayName": "Amanuel Alambo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1kA5oHs9X8VTfaoRu8wK35Yj33QD7CJkOZ6QBgQ=s64",
      "userId": "11985467428571358535"
     },
     "user_tz": 300
    },
    "id": "9mvoeHMHB0bx",
    "outputId": "3594f573-23a6-4a62-a2e7-452d03557ee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement graphviz-dev (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for graphviz-dev\u001b[0m\n",
      "  Building wheel for pygraphviz (setup.py) ... \u001b[?25lerror\n",
      "\u001b[31m  ERROR: Failed building wheel for pygraphviz\u001b[0m\n",
      "\u001b[?25h    Running setup.py install for pygraphviz ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-nrvqfhlm/pygraphviz_1a56e1e5e55343df9c5eff814aa8f2a3/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-nrvqfhlm/pygraphviz_1a56e1e5e55343df9c5eff814aa8f2a3/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-3yqivu0v/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/pygraphviz Check the logs for full command output.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -q graphviz\n",
    "!pip3 install -q graphviz-dev\n",
    "!pip3 install -q pygraphviz\n",
    "!pip3 install -q networkx==1.9.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1636992741571,
     "user": {
      "displayName": "Amanuel Alambo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1kA5oHs9X8VTfaoRu8wK35Yj33QD7CJkOZ6QBgQ=s64",
      "userId": "11985467428571358535"
     },
     "user_tz": 300
    },
    "id": "oL5SCc5EA2ze",
    "outputId": "6c46f346-d1ff-42e9-a311-8d7998d773dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/apex-codes/citation_sum\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/apex-codes/citation_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1636992741571,
     "user": {
      "displayName": "Amanuel Alambo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1kA5oHs9X8VTfaoRu8wK35Yj33QD7CJkOZ6QBgQ=s64",
      "userId": "11985467428571358535"
     },
     "user_tz": 300
    },
    "id": "aJ-tHSXzBT5Y",
    "outputId": "ed373f80-72a2-45a3-dac2-c7c5ac8f76f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/apex-codes/citation_sum/takahe/takahe\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/apex-codes/citation_sum/takahe/takahe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOLu-FH_amd4"
   },
   "outputs": [],
   "source": [
    "!pip3 install -q -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1636992744062,
     "user": {
      "displayName": "Amanuel Alambo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1kA5oHs9X8VTfaoRu8wK35Yj33QD7CJkOZ6QBgQ=s64",
      "userId": "11985467428571358535"
     },
     "user_tz": 300
    },
    "id": "0UUHm-3FAz4J",
    "outputId": "e2958b23-53f2-4fc2-ea21-8ba9107d6730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import takahe\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize\n",
    "from nltk import tokenize\n",
    "from pprint import pprint\n",
    "import json\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial.distance import cosine\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "65Qw49jEMw2v"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_REqrNvZg7n"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHArlDjEasx8"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rp7iMfkQY8A5"
   },
   "outputs": [],
   "source": [
    "# Function to compute abstractiveness score of a path from a word grah\n",
    "# ---accepts a list of candidate paths and returns a score for each path\n",
    "def _compute_abstractiveness(final_selected_paths, sentences_in_cluster):\n",
    "  # A path should be semantically well representing the citation contexts (i.e, the sentences in the cluster of citation contexts) forming the word graph\n",
    "  cluster_sentences_embeddings = model.encode(sentences_in_cluster)  # generate embeddings\n",
    "  dict_path_cosine_scores = {}  # path as key and value as the aggregate cosine similarity score against the ACCs\n",
    "  # iterate through the paths in final_selected_paths and compute cosine against the cluster_sentences_embeddings to measure semantic score\n",
    "  for path in final_selected_paths:\n",
    "    path_cos_score = 0.0\n",
    "    path_embedding = model.encode([path])[0]\n",
    "    for sentence_in_cluster_embed in cluster_sentences_embeddings:\n",
    "      path_cos_score += cosine(path_embedding.reshape(-1,1), sentence_in_cluster_embed.reshape(-1,1))\n",
    "    dict_path_cosine_scores[str(path)] = path_cos_score\n",
    "\n",
    "  return dict_path_cosine_scores\n",
    "\n",
    "# New sentence pos tagger function\n",
    "def sentence_pos_tagger(sentences):\n",
    "  tagged_sentences = []\n",
    "  for sent in sentences:\n",
    "    if sent != '':\n",
    "      sent = sp(sent)\n",
    "      tagged_words = []\n",
    "      for word in sent:\n",
    "        word_tag = word.tag_\n",
    "        tagged_words.append(str(word) + \"/\" + str(word_tag))\n",
    "      tagged_words = \" \".join(tagged_words)\n",
    "      tagged_sentences.append(tagged_words)\n",
    "\n",
    "  return tagged_sentences\n",
    "\n",
    "# Function to generate a compressed path\n",
    "def generate_paths(acc, no_paths_in_wg):\n",
    "  # Now run MSC for each item in the list\n",
    "  tagged_sentences = sentence_pos_tagger(acc)\n",
    "  compresser = takahe.word_graph(tagged_sentences,\n",
    "                                 nb_words = 8,\n",
    "                                 lang='en',\n",
    "                                 punct_tag='PUNCT')\n",
    "  \n",
    "  # Get the 50 best paths\n",
    "  candidates = compresser.get_compression(50)\n",
    "  final = []\n",
    "  for c in candidates:\n",
    "      final.append(c[1:])\n",
    "  selected_path = ' '.join([item[0] for item in final[0][0]])\n",
    "  final_selected_paths = list()\n",
    "  for path in final:\n",
    "    selected_path = ' '.join([item[0] for item in path[0]])\n",
    "    final_selected_paths.append(selected_path)\n",
    "  \n",
    "  # Now pick the paths based on abstractiveness_score and redundancy----don't pick a path that is highly semantically similar to an already selected path\n",
    "  dict_path_scores = _compute_abstractiveness(final_selected_paths, acc)\n",
    "  dict_path_scores = dict(sorted(dict_path_scores.items(), key=operator.itemgetter(1), reverse=True))\n",
    "  \n",
    "  return dict_path_scores   # From the calling program, grab the keys which are the paths, to form the final summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLNLMx1-aFXv"
   },
   "source": [
    "## Iterate through each cited_id json file and form a word graph corresponding to each cluster of abstractive citation contexts----determine the number of paths to grab from a word graph based on the 250-word limit(e.g., 250/word graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1636992765475,
     "user": {
      "displayName": "Amanuel Alambo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1kA5oHs9X8VTfaoRu8wK35Yj33QD7CJkOZ6QBgQ=s64",
      "userId": "11985467428571358535"
     },
     "user_tz": 300
    },
    "id": "ThVYQ0bmz2fn",
    "outputId": "5c103758-7c0d-4bf9-bef2-d898eb285c3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/apex-codes/citation_sum/takahe/takahe\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaRv14t40xHu"
   },
   "source": [
    "## Function to generate final summaries for each cited paper using TransFuse. Note to order the final summary such that most informative sentences come at the beginning followed by less informative sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeLJUZ3WakHT"
   },
   "outputs": [],
   "source": [
    "def _generate_TransFuse_summary(ABSTRACTIVE_CITATION_CONTEXT_CLUSTERS_PATH, OUTPUT_PATH, w_abstract=False, cossim_threshold=0.7):\n",
    "  for iter, cited_id in enumerate(os.listdir(ABSTRACTIVE_CITATION_CONTEXT_CLUSTERS_PATH)):\n",
    "    timeout_sec = datetime.timedelta(seconds=30)\n",
    "    if iter > 240:\n",
    "      print(\"Iteration: {} -- cited_id {}\".format(iter, cited_id))\n",
    "      final_complete_summary_paths = []   # This is for the entire list of paths from all clusters for a cited paper\n",
    "      CITED_JSON_PATH = os.path.join(ABSTRACTIVE_CITATION_CONTEXT_CLUSTERS_PATH, cited_id)\n",
    "      with open(CITED_JSON_PATH, 'r') as fp:\n",
    "        dict_acc = json.load(fp)\n",
    "      fp.close()\n",
    "      #print('Number of clusters:', len(dict_acc))    # a cluster has a set of abstractive citation contexts\n",
    "      no_of_clusters = len(dict_acc)   # a word graph is built for each cluster\n",
    "      # Determine the number of paths to be selected from a word graph\n",
    "      no_paths_in_wg = int(250/(no_of_clusters * 10))   # The '200' is the expected number of words in a summary. '10' is number of words in English sentence\n",
    "      #print('Number of paths to select from a single word graph: ', no_paths_in_wg, '\\n')\n",
    "\n",
    "      total_candidate_paths = []   # stores the best scoring paths' dict from all the word graphs--later the algorithm of generating the final summary runs\n",
    "      for cluster_id, acc_list in dict_acc.items():\n",
    "        sentences_in_cluster = sent_tokenize(' '.join(acc_list))\n",
    "        try:\n",
    "          dict_path_scores = generate_paths(sentences_in_cluster, no_paths_in_wg)   # {path-1:18.xx, path-2:19.yy}\n",
    "        except:\n",
    "          continue  \n",
    "        total_candidate_paths.append(dict_path_scores)  \n",
    "\n",
    "      # Iterate through the total_candidate_paths list and extract the paths to form the final summary---the item iterated on is a dict\n",
    "      #final_summary_paths = []   # This is per word graph\n",
    "      for dict_path_w_score in total_candidate_paths:\n",
    "        final_summary_paths = []   # This is per word graph\n",
    "        for path, score in dict_path_w_score.items():\n",
    "          #print(path)\n",
    "          flag = 0\n",
    "          if len(final_summary_paths) == no_paths_in_wg:\n",
    "            break\n",
    "          elif len(final_summary_paths) > 0:\n",
    "            # encode the path using sentence-BERT\n",
    "            path_embedding = model.encode([path])  # generate embedding for a path\n",
    "            final_summary_paths_embedding = model.encode(final_summary_paths)   # embeddings for the paths already selected\n",
    "            # chose a semantic similarity threshold of 0.8 to decide how similar a path and a sentence in the selected list is\n",
    "            for idx, path_in_summary_embedding in enumerate(final_summary_paths_embedding):\n",
    "              #sim_score = cosine(path_embedding.reshape(-1,1), path_in_summary_embedding.reshape(-1,1))\n",
    "              sim_score = cosine(path_embedding, path_in_summary_embedding)\n",
    "              if sim_score >= cossim_threshold:\n",
    "                # turn a flag on that insertion is handled here\n",
    "                flag = 1\n",
    "                # replace the one present in the summary with the one coming on if the coming one has higher score in abstractiveness\n",
    "                path_in_summary = final_summary_paths[idx]   # grab the path in the summary list that is highly similar with the candidate one\n",
    "                path_in_summary_score = dict_path_w_score[path_in_summary]   # grab the score from the dictionary\n",
    "                # get the score of the newly coming path in the dictionary to compare with the one already present\n",
    "                path_incoming_score = dict_path_w_score[path]\n",
    "                if path_incoming_score > path_in_summary_score:\n",
    "                  # Perform replacement\n",
    "                  final_summary_paths[idx] = path\n",
    "                  break\n",
    "          if flag == 0:\n",
    "            final_summary_paths.append(path)\n",
    "          \n",
    "        final_complete_summary_paths.append(final_summary_paths)\n",
    "      final_complete_summary_paths = [item for subl in final_complete_summary_paths for item in subl]   # Flattening list of lists\n",
    "      final_summary = \" \".join(final_complete_summary_paths)\n",
    "\n",
    "      cited_id_wo_json = cited_id.replace('.json', '')\n",
    "      with open(f\"{OUTPUT_PATH}/{cited_id_wo_json}.txt\", 'w') as fp:\n",
    "        fp.write(final_summary)\n",
    "      fp.close()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNJbUFK8TF6T"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  w_abstract = True\n",
    "  if w_abstract:\n",
    "    ABSTRACTIVE_CITATION_CONTEXT_CLUSTERS_PATH = f\"../../CLUSTERING_SciSummNet_Abstractive_Citation_Contexts/CLUSTERS_FROM_CITATIONS_AND_RP_ABSTRACT\"\n",
    "  else:\n",
    "    ABSTRACTIVE_CITATION_CONTEXT_CLUSTERS_PATH = f\"../../CLUSTERING_SciSummNet_Abstractive_Citation_Contexts/CLUSTERS_FROM_CITATIONS_ONLY\"\n",
    "\n",
    "  if not w_abstract:\n",
    "    SUMMARY_TYPE = \"SUMMARIES_FROM_CITATIONS_ONLY\" # changes with the input configuration (w/wo RP abstract)\n",
    "  else:\n",
    "    SUMMARY_TYPE = \"SUMMARIES_FROM_CITATIONS_AND_RP_ABSTRACT\"\n",
    "\n",
    "  OUTPUT_PATH = f\"../../TransFuse_Results_SciSummNet/{SUMMARY_TYPE}\"\n",
    "  \n",
    "  os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "  # call to the summary generation method above\n",
    "  _generate_TransFuse_summary(ABSTRACTIVE_CITATION_CONTEXT_CLUSTERS_PATH, OUTPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6661,
     "status": "ok",
     "timestamp": 1636992782882,
     "user": {
      "displayName": "Amanuel Alambo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1kA5oHs9X8VTfaoRu8wK35Yj33QD7CJkOZ6QBgQ=s64",
      "userId": "11985467428571358535"
     },
     "user_tz": 300
    },
    "id": "jNqGp_c1TF4f",
    "outputId": "0594ace7-7f9b-4c0d-bd5e-7699a6af4d56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 241 -- cited_id J97-1002.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0F1Ls6a2TF2U"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MSC_over_Clusters_SciSummNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
