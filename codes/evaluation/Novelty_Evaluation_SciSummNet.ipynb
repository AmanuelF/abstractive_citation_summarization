{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23229,
     "status": "ok",
     "timestamp": 1636994215712,
     "user": {
      "displayName": "Amanuel Alambo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1kA5oHs9X8VTfaoRu8wK35Yj33QD7CJkOZ6QBgQ=s64",
      "userId": "11985467428571358535"
     },
     "user_tz": 300
    },
    "id": "EcUFNkHuN-F7",
    "outputId": "6db53be0-cff9-430d-95bf-8c6f235b46e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Run in python console\n",
    "import nltk; nltk.download('stopwords')\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1504,
     "status": "ok",
     "timestamp": 1636994220470,
     "user": {
      "displayName": "Amanuel Alambo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1kA5oHs9X8VTfaoRu8wK35Yj33QD7CJkOZ6QBgQ=s64",
      "userId": "11985467428571358535"
     },
     "user_tz": 300
    },
    "id": "K5ZiHokz_c4w",
    "outputId": "b3e73672-c55f-4827-9cb9-deefe1880ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/apex-codes/citation_sum\n"
     ]
    }
   ],
   "source": [
    "%cd drive/My\\ Drive/Colab\\ Notebooks/apex-codes/citation_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 979,
     "status": "ok",
     "timestamp": 1636994225992,
     "user": {
      "displayName": "Amanuel Alambo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1kA5oHs9X8VTfaoRu8wK35Yj33QD7CJkOZ6QBgQ=s64",
      "userId": "11985467428571358535"
     },
     "user_tz": 300
    },
    "id": "eRmPOi71aZIN",
    "outputId": "66e74872-b728-4bf7-8a84-85659a594803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZvwjL8qHb83"
   },
   "source": [
    "## Compute Novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7890,
     "status": "ok",
     "timestamp": 1636994240952,
     "user": {
      "displayName": "Amanuel Alambo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1kA5oHs9X8VTfaoRu8wK35Yj33QD7CJkOZ6QBgQ=s64",
      "userId": "11985467428571358535"
     },
     "user_tz": 300
    },
    "id": "2ZG5yw0WaKCL",
    "outputId": "1153fca2-6cb0-4890-bb88-bad6da494af1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 5.3 MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1670,
     "status": "ok",
     "timestamp": 1636994245068,
     "user": {
      "displayName": "Amanuel Alambo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1kA5oHs9X8VTfaoRu8wK35Yj33QD7CJkOZ6QBgQ=s64",
      "userId": "11985467428571358535"
     },
     "user_tz": 300
    },
    "id": "E1hWKRq_MFDm"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import string\n",
    "import json\n",
    "import spacy\n",
    "import nltk\n",
    "import gensim\n",
    "import collections\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "warnings.filterwarnings('ignore')\n",
    "from gensim.models import Phrases\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "stopwords = str(stop_words)\n",
    "stop = open('terrier-stop.txt')\n",
    "stopString = stop.read()\n",
    "common_terms = stopString.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4hQcKXAXV6x"
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1636994249191,
     "user": {
      "displayName": "Amanuel Alambo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1kA5oHs9X8VTfaoRu8wK35Yj33QD7CJkOZ6QBgQ=s64",
      "userId": "11985467428571358535"
     },
     "user_tz": 300
    },
    "id": "elMIfXuxXTpA"
   },
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def preprocess_data(data):\n",
    "  data = data\n",
    "  # Remove Emails\n",
    "  data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "  # Remove new line characters\n",
    "  data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "  # Remove distracting single quotes\n",
    "  data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "  return data\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts, bigram_mod):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts, bigram_mod, trigram_mod):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    #!python3 -m spacy download en\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "def nlp_process(data_words, bigram_mod, trigram_mod):\n",
    "  # Remove Stop Words\n",
    "  data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "  # Form Bigrams\n",
    "  data_words_bigrams = make_bigrams(data_words_nostops, bigram_mod)\n",
    "\n",
    "  # Do lemmatization keeping only noun, adj, vb, adv\n",
    "  data_lemmatized = lemmatization(data_words_bigrams)\n",
    "\n",
    "  return data_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jANt0lXDPLk3"
   },
   "source": [
    "### Unigrams generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1636994251892,
     "user": {
      "displayName": "Amanuel Alambo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1kA5oHs9X8VTfaoRu8wK35Yj33QD7CJkOZ6QBgQ=s64",
      "userId": "11985467428571358535"
     },
     "user_tz": 300
    },
    "id": "3V0dW6GoL72n"
   },
   "outputs": [],
   "source": [
    "# This function returns unigrams for 1) abstract or 2) the collection of topic-aware citation contexts---so it must be passed the raw abstrac\n",
    "# or the raw citation contexts---this one generates top 50 unigrams\n",
    "def _generate_unigrams(text):\n",
    "  unigrams_list = []\n",
    "\n",
    "  words = nltk.word_tokenize(text)\n",
    "  fileList = [word.lower() for word in words if word.isalpha()]\n",
    "  freeList = [t for t in fileList if t not in common_terms and t not in stop_words and t not in string.punctuation]\n",
    "  freeString = [t for t in fileList if t not in common_terms and t not in stop_words and t not in string.punctuation]\n",
    "  a = ' '.join(freeString)       #conversion into a string\n",
    "  wordcount = {}\n",
    "  for word in freeList:      \n",
    "      if word not in stopwords:\n",
    "          if word not in wordcount:\n",
    "              wordcount[word] = 1\n",
    "          else:\n",
    "              wordcount[word] += 1\n",
    "  word_counter = collections.Counter(wordcount)\n",
    "  # Just grab the 50 most frequently occuring unigrams---number can change accordingly\n",
    "  for word, count in word_counter.most_common(50):\n",
    "      unigrams_list.append(word)\n",
    "\n",
    "  return unigrams_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IM6ru1BOPP2u"
   },
   "source": [
    "### Bigrams and trigrams generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 333,
     "status": "ok",
     "timestamp": 1636994254436,
     "user": {
      "displayName": "Amanuel Alambo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1kA5oHs9X8VTfaoRu8wK35Yj33QD7CJkOZ6QBgQ=s64",
      "userId": "11985467428571358535"
     },
     "user_tz": 300
    },
    "id": "Q6UMjol9PR7P"
   },
   "outputs": [],
   "source": [
    "def _generate_bigrams_trigrams(text):\n",
    "  #bigrams_list = []\n",
    "  # tokenize the text into individual sentences that are stored in a list\n",
    "  filtered_data = sent_tokenize(text)\n",
    "\n",
    "  data = preprocess_data(filtered_data)\n",
    "\n",
    "  #######################################################################################\n",
    "  data_words = list(sent_to_words(data))\n",
    "\n",
    "\n",
    "  # Build the bigram and trigram models\n",
    "  bigram = gensim.models.Phrases(data_words, min_count=1, threshold=1) # higher threshold fewer phrases.\n",
    "  trigram = gensim.models.Phrases(bigram[data_words], min_count=1, threshold=1)  \n",
    "\n",
    "  # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "  bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "  trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "  data_lemmatized = nlp_process(data_words, bigram_mod, trigram_mod)\n",
    "\n",
    "  sentence_stream = data_lemmatized\n",
    "\n",
    "  bigrams_lemmatized = []\n",
    "  trigrams_lemmatized = []\n",
    "\n",
    "  bigram = Phrases(sentence_stream, min_count=3, delimiter=b'_')\n",
    "  trigram  = Phrases(bigram[sentence_stream], min_count=3, delimiter=b'_')\n",
    "  for sent in sentence_stream:\n",
    "      bigrams_ = [b for b in bigram[sent] if b.count('_') == 1]\n",
    "      trigrams_ = [t for t in trigram[bigram[sent]] \n",
    "                                            if t.count('_')==2]\n",
    "      bigrams_lemmatized.append(bigrams_)\n",
    "      trigrams_lemmatized.append(trigrams_)\n",
    "  \n",
    "  ngrams_lemmatized =  trigrams_lemmatized + bigrams_lemmatized\n",
    "\n",
    "  return bigrams_lemmatized, trigrams_lemmatized\n",
    "  #return ngrams_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEj3bMUON2v0"
   },
   "source": [
    "# Compute Novelty with reference to gound truth summaries\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 427,
     "status": "ok",
     "timestamp": 1636994258554,
     "user": {
      "displayName": "Amanuel Alambo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1kA5oHs9X8VTfaoRu8wK35Yj33QD7CJkOZ6QBgQ=s64",
      "userId": "11985467428571358535"
     },
     "user_tz": 300
    },
    "id": "1jWqUvQUNP3V"
   },
   "outputs": [],
   "source": [
    "def _compute_novelty_I(gen_summary, human_summary):\n",
    "  # generate unigrams for the summary and the abstract\n",
    "  gen_summary_unigrams = _generate_unigrams(gen_summary)\n",
    "  human_summary_unigrams = _generate_unigrams(human_summary)\n",
    "\n",
    "  # generate bigrams and trigrams for the summary and the abstract\n",
    "  gen_summary_bi_trigrams, gen_summary_tri_trigrams = _generate_bigrams_trigrams(gen_summary)\n",
    "  human_summary_bi_trigrams, human_summary_tri_trigrams = _generate_bigrams_trigrams(human_summary)\n",
    "\n",
    "  # Total n-grams for the summary and the abstract\n",
    "  gen_summary_ngrams = gen_summary_unigrams + gen_summary_bi_trigrams + gen_summary_tri_trigrams\n",
    "  human_summary_ngrams = human_summary_unigrams + human_summary_bi_trigrams + human_summary_tri_trigrams\n",
    "\n",
    "\n",
    "  diff_list = [ngram for ngram in gen_summary_ngrams if ngram not in human_summary_ngrams]   # unigram that is in a summary but not in the abstract\n",
    "\n",
    "  novelty_score = float(len(diff_list)) / float(len(gen_summary_ngrams))\n",
    "  novelty_score = novelty_score * 100\n",
    "\n",
    "  return novelty_score\n",
    "\n",
    "  #print(f\"{RESULTS_PATH} Novelty Score wrt abstracts: %.2f\" % novelty_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S254gwTvrWEP"
   },
   "source": [
    "## Evaluation against the citation contexts themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1636994367519,
     "user": {
      "displayName": "Amanuel Alambo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1kA5oHs9X8VTfaoRu8wK35Yj33QD7CJkOZ6QBgQ=s64",
      "userId": "11985467428571358535"
     },
     "user_tz": 300
    },
    "id": "iyymT1KfSr80"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "  # Read the citation contexts for each reference paper\n",
    "  model_name = \"TransFuse\"  # changes with the model used\n",
    "  w_citing_sentences_w_rp_abstract = True # Turn on or off based on whether to have RP abstract as input\n",
    "\n",
    "  if w_citing_sentences_w_rp_abstract:\n",
    "    summary_type = \"SUMMARIES_FROM_CITATIONS_AND_RP_ABSTRACT\"\n",
    "  else:\n",
    "    summary_type = \"SUMMARIES_FROM_CITATIONS_ONLY\"   # changes with the summary type\n",
    "\n",
    "  GENERATED_SUMMARY_PATH = f\"{model_name}_Results_SciSummNet/{summary_type}\"\n",
    "  HUMAN_SUMMARY_PATH = \"ScisummNet/scisummnet_release1.1__20190413/top1000_complete\"\n",
    "\n",
    "  # First read all the generated summaries into a dict where the key is the paper_id---same for human summaries---the keys will be used to match the two\n",
    "  dict_generated_summaries = {}\n",
    "  dict_citation_contexts = {}\n",
    "\n",
    "  # Read all the generated summaries\n",
    "  for paper_id in os.listdir(GENERATED_SUMMARY_PATH):\n",
    "    paper_id_wo_txt = str(paper_id.replace('.txt', ''))\n",
    "    with open(os.path.join(GENERATED_SUMMARY_PATH, paper_id), 'r') as fp:\n",
    "      summary = fp.read()\n",
    "    fp.close()\n",
    "    dict_generated_summaries[paper_id_wo_txt] = summary\n",
    "\n",
    "  DATA_PATH = \"ScisummNet/scisummnet_release1.1__20190413/top1000_complete\"\n",
    "  RESULTS_DIR = f\"{model_name}_Results_SciSummNet\"   # changes based on the model currently being used\n",
    "  #w_citing_sentences_w_rp_abstract = True   # changes according to input configuration (if False, it is with citation contexts only)\n",
    "  dict_citation_contexts = {}\n",
    "\n",
    "  for iter, paper_id in enumerate(os.listdir(DATA_PATH)):\n",
    "    if iter % 200 == 0:\n",
    "      print(\"Iteration: {}\".format(iter))\n",
    "    citing_sentences = list()   # to store all incoming citing sentences\n",
    "    for file in os.listdir(os.path.join(DATA_PATH, paper_id)):\n",
    "      if file.endswith('.json'):\n",
    "        with open(os.path.join(f\"{DATA_PATH}/{paper_id}\", file), 'r') as fp:\n",
    "          data = json.load(fp)\n",
    "        fp.close()\n",
    "        citing_sentences = [obj['clean_text'] for obj in data]\n",
    "\n",
    "    complete_citing_sentences_str = \" \".join(citing_sentences)\n",
    "    dict_citation_contexts[paper_id] = complete_citing_sentences_str\n",
    "\n",
    "  ## Call to the the novelty computing method\n",
    "  total_no_summaries = len(dict_generated_summaries)\n",
    "\n",
    "  novelty_sum = 0.0\n",
    "  for paper_id, gen_summary in dict_generated_summaries.items():\n",
    "    citation_contexts = dict_citation_contexts[paper_id]\n",
    "\n",
    "    # call to the novelty evaluation method\n",
    "    try:\n",
    "      novelty = _compute_novelty_I(gen_summary, citation_contexts)\n",
    "    except:\n",
    "      continue\n",
    "\n",
    "    novelty_sum += novelty\n",
    "\n",
    "  avg_novelty = novelty_sum/float(total_no_summaries)\n",
    "\n",
    "  print(f\"\\nFinal Novelty for summaries using {model_name} and {summary_type} wrt citation contexts: %.2f\" % avg_novelty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 941679,
     "status": "ok",
     "timestamp": 1636995311567,
     "user": {
      "displayName": "Amanuel Alambo",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi1kA5oHs9X8VTfaoRu8wK35Yj33QD7CJkOZ6QBgQ=s64",
      "userId": "11985467428571358535"
     },
     "user_tz": 300
    },
    "id": "Y_XAOELKheRd",
    "outputId": "2e0c873d-ff7d-4131-e95a-f6aa7f8ae33c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Iteration: 200\n",
      "Iteration: 400\n",
      "Iteration: 600\n",
      "Iteration: 800\n",
      "Iteration: 1000\n",
      "\n",
      "Final Novelty for summaries using TransFuse and SUMMARIES_FROM_CITATIONS_AND_RP_ABSTRACT wrt citation contexts: 45.79\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEQC5XmGsdPy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCUhQWuCsdHc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Novelty_Evaluation_SciSummNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
