{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LS0BC5oErcAP",
    "outputId": "e7f4d5da-7b0e-4fea-f10c-7be56f9c7149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sT8qQNCSreSW",
    "outputId": "96cab8f2-fbe8-4362-8e7e-66bab7692327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/apex-codes/citation_sum\n"
     ]
    }
   ],
   "source": [
    "%cd drive/My\\ Drive/Colab\\ Notebooks/apex-codes/citation_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZaDRjfTmey1o",
    "outputId": "dd2df4d7-f0fd-45bf-a258-06ba3b036f83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_gaD66LKu3f"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "from scipy.spatial.distance import cosine\n",
    "from gensim.test.utils import common_texts\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAWBAicUjs4W"
   },
   "source": [
    "## https://github.com/allenai/paper-embedding-public-apis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OkVc3aZ2uDN"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import json\n",
    "import requests\n",
    "\n",
    "URL = \"https://model-apis.semanticscholar.org/specter/v1/invoke\"\n",
    "MAX_BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "def chunks(lst, chunk_size=MAX_BATCH_SIZE):\n",
    "    \"\"\"Splits a longer list to respect batch size\"\"\"\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        yield lst[i : i + chunk_size]\n",
    "\n",
    "def embed(papers):\n",
    "    embeddings_by_paper_id: Dict[str, List[float]] = {}\n",
    "\n",
    "    for chunk in chunks(papers):\n",
    "        # Allow Python requests to convert the data above to JSON\n",
    "        response = requests.post(URL, json=chunk)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise RuntimeError(\"Sorry, something went wrong, please try later!\")\n",
    "\n",
    "        for paper in response.json()[\"preds\"]:\n",
    "            embeddings_by_paper_id[paper[\"paper_id\"]] = np.array(paper[\"embedding\"])\n",
    "\n",
    "    return embeddings_by_paper_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVzEGwIWfC6G"
   },
   "source": [
    "# Working with SPECTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ph9Fd5Rtyy1"
   },
   "outputs": [],
   "source": [
    "%mkdir SPECTER_FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SRepAKDc-Riu",
    "outputId": "526c037b-3afc-4a65-fd31-2c1dbd1af3b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/apex-codes/citation_sum\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgiRlf4uRpDX"
   },
   "source": [
    "## Function to read an fos and generate SPECTER embedding for each abstract and write to a file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQRX12-8EwaU"
   },
   "outputs": [],
   "source": [
    "# Function to read an fos and generate SPECTER embedding for each abstract and write to a file system\n",
    "def _generate_SPECTER_embed_abstract(fos):\n",
    "  df = pd.read_csv(f'../100_most_cited_w_titles/{fos}.csv')\n",
    "  rp_abstract_lst = []\n",
    "  \n",
    "  for idx, row in df.iterrows():\n",
    "    rp_abstract_lst.append({\"paper_id\":str(df['paper_id'][idx]), \n",
    "                            \"title\":df['paper_title'][idx], \n",
    "                            \"abstract\":df['paper_abstract'][idx]})\n",
    "    \n",
    "  all_embeddings = embed(rp_abstract_lst)   # Generate embeddings for the abstracts\n",
    "\n",
    "  pk.dump(all_embeddings, open(f\"SPECTER_FILES/RP_abstract_embeddings/{fos}.pk\", \"wb\"))  # write to the file system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRnWZ0EMFrEg"
   },
   "outputs": [],
   "source": [
    "fos = \"data_mining\"\n",
    "_generate_SPECTER_embed_abstract(fos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edbb8VT4ReeN"
   },
   "source": [
    "## Function to generate SPECTER embeddings for each topic-aware/topic-focused citation contexts. Generate a similar data structure as created for the abstracts for each fos---just replace abstracts with a bunch of TaCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_ZNsiXZF4HO"
   },
   "outputs": [],
   "source": [
    "def _generate_SPECTER_embed_TaCC(fos):\n",
    "  #fos = \"artificial_intelligence\"\n",
    "  TOPIC_FOCUSED_CITATION_CONTEXT_PATH = f\"100_highest_ranked_citing_papers/citation_contexts_TOPIC_FOCUSED/{fos}\"\n",
    "\n",
    "  df_title_w_paper_id = pd.read_csv(f'../100_most_cited_w_titles/{fos}.csv')\n",
    "\n",
    "  TaCC_lst = []\n",
    "  for cited_id in os.listdir(TOPIC_FOCUSED_CITATION_CONTEXT_PATH):\n",
    "      df = pd.read_csv(os.path.join(TOPIC_FOCUSED_CITATION_CONTEXT_PATH, cited_id))\n",
    "      # Grab topic-focused citation contexts for a cited article\n",
    "      if len(df) != 0:\n",
    "        try:\n",
    "          topic_citation_contexts_list = df['topic_focused_citation_context'].values\n",
    "        except:\n",
    "          continue   \n",
    "        topic_citation_contexts_final = list()\n",
    "        for citation_context in topic_citation_contexts_list:\n",
    "          try:\n",
    "            topic_citation_contexts_final.append(citation_context.replace(\"'\", '').replace(\".,\", '.').replace('[', '').replace(']',''))\n",
    "          except:\n",
    "            continue\n",
    "\n",
    "        topic_citation_contexts_final = ' '.join(topic_citation_contexts_final)\n",
    "        cited_id_wo_csv = cited_id.replace('.csv','')\n",
    "        #print(cited_id_wo_csv)\n",
    "        paper_title = df_title_w_paper_id[df_title_w_paper_id['paper_id'] == int(cited_id_wo_csv)]['paper_title'].values\n",
    "        \n",
    "        paper_title = paper_title[0]\n",
    "\n",
    "        TaCC_lst.append({\"paper_id\":str(cited_id_wo_csv),\n",
    "                        \"title\":paper_title,\n",
    "                        \"abstract\":topic_citation_contexts_final})\n",
    "      \n",
    "  all_embeddings = embed(TaCC_lst)   # Generate embeddings for the abstracts\n",
    "\n",
    "  pk.dump(all_embeddings, open(f\"SPECTER_FILES/TaCC_embeddings/{fos}.pk\", \"wb\"))  # write to the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENhJGCZ6Pctd"
   },
   "outputs": [],
   "source": [
    "fos = \"machine_learning\"\n",
    "_generate_SPECTER_embed_TaCC(fos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQbr80oQRsP6"
   },
   "source": [
    "## Function to generate SPECTER embeddings for each class of summaries (i.e., using the different models---e.g., T5, BART, Pegasus, etc). Note that in this function, in addition to the fos, the function accepts the summary_path also, whether T5, BART, Pegasus, or so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ffideEJR3O9"
   },
   "outputs": [],
   "source": [
    "def _generate_SPECTER_embed_summary(fos, summary_path, summary_embed_path):\n",
    "  df_title_w_paper_id = pd.read_csv(f'../100_most_cited_w_titles/{fos}.csv')\n",
    "  summary_lst = []\n",
    "\n",
    "  for cited_id in os.listdir(summary_path):\n",
    "    with open(os.path.join(summary_path, cited_id), 'r') as fp:\n",
    "      summary = fp.read()\n",
    "    fp.close()\n",
    "\n",
    "    cited_id_wo_csv = cited_id.replace('.csv', '')\n",
    "\n",
    "    paper_title = df_title_w_paper_id[df_title_w_paper_id['paper_id'] == int(cited_id_wo_csv)]['paper_title'].values    \n",
    "    paper_title = paper_title[0]\n",
    "\n",
    "    summary_lst.append({\"paper_id\":str(cited_id_wo_csv),\n",
    "                        \"title\":paper_title,\n",
    "                        \"abstract\":summary})\n",
    "    \n",
    "  all_embeddings = embed(summary_lst)   # Generate embeddings for the abstracts\n",
    "\n",
    "  pk.dump(all_embeddings, open(f\"SPECTER_FILES/{summary_embed_path}/{fos}.pk\", \"wb\"))  # write to the file system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLemKPRLQo-c"
   },
   "outputs": [],
   "source": [
    "fos_lst = [\"artificial_intelligence\", \"machine_learning\", \"data_mining\"]\n",
    "summary_type = \"MAGSumm_Results_w_title_and_topic_augmentation\"\n",
    "summary_embed_path = \"MAGSumm_Summary_w_title_w_topic_embeddings\"\n",
    "\n",
    "for fos in fos_lst:\n",
    "  SUMMARY_PATH = f\"{summary_type}/{fos}\"\n",
    "  _generate_SPECTER_embed_summary(fos, SUMMARY_PATH, summary_embed_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YoEDeBntZiS0"
   },
   "outputs": [],
   "source": [
    "fos = \"artificial_intelligence\"\n",
    "RP_ABSTRACT_EMBED_PATH = pk.load(open(f\"SPECTER_FILES/RP_abstract_embeddings/{fos}.pk\", \"rb\"))  # read from the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gefTMJ6oZ9oM",
    "outputId": "d9e564c6-6405-44f2-f6bd-bb9270ead50d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 97,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RP_ABSTRACT_EMBED_PATH['2964299589'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3Hq3708bqTJ"
   },
   "source": [
    "## Semantic Equivalence between summaries and abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RnDw_82NaUeO"
   },
   "outputs": [],
   "source": [
    "def _compute_cosine_summary_w_abstract(fos, summary_embed_path):\n",
    "  DICT_RP_ABSTRACT_EMBED = pk.load(open(f\"SPECTER_FILES/RP_abstract_embeddings/{fos}.pk\", \"rb\"))  # write to the file system\n",
    "  DICT_SUMMARY = pk.load(open(f\"SPECTER_FILES/{summary_embed_path}/{fos}.pk\", \"rb\"))  # write to the file system\n",
    "\n",
    "  total_cosine_sum = 0.0\n",
    "  count = 0\n",
    "\n",
    "  for paper_id, summary_embed in DICT_SUMMARY.items():\n",
    "    rp_abstract_embed = DICT_RP_ABSTRACT_EMBED[paper_id]\n",
    "    total_cosine_sum += cosine(summary_embed, rp_abstract_embed)\n",
    "    count += 1\n",
    "  return total_cosine_sum/float(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fos_lst = [\"artificial_intelligence\", \"machine_learning\", \"data_mining\"]\n",
    "\n",
    "summary_embed_path_list = [\"T5_Summary_w_title_w_topic_embeddings\", \"BART_Summary_w_title_w_topic_embeddings\", \n",
    "                           \"Pegasus_Summary_w_title_w_topic_embeddings\", \"ProphetNet_Summary_w_title_w_topic_embeddings\", \n",
    "                           \"MAGSumm_Summary_w_title_w_topic_embeddings\"]\n",
    "for fos in fos_lst:\n",
    "  print(f\"Results for {fos} \\n\")\n",
    "  for summary_embed_path in summary_embed_path_list:\n",
    "    avg_cosine = _compute_cosine_summary_w_abstract(fos, summary_embed_path) * 100\n",
    "    print(f\"{summary_embed_path} cosine wrt RP abstracts: %.2f\" % avg_cosine)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ol64weRc3PL"
   },
   "source": [
    "## Semantic Equivalence between summaries and TaCC from which the summaries are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDeZIFthc7ad"
   },
   "outputs": [],
   "source": [
    "def _compute_cosine_summary_w_TaCC(fos, summary_embed_path):\n",
    "  DICT_TaCC_EMBED = pk.load(open(f\"SPECTER_FILES/TaCC_embeddings/{fos}.pk\", \"rb\"))  # write to the file system\n",
    "  DICT_SUMMARY = pk.load(open(f\"SPECTER_FILES/{summary_embed_path}/{fos}.pk\", \"rb\"))  # write to the file system\n",
    "\n",
    "  total_cosine_sum = 0.0\n",
    "  count = 0\n",
    "\n",
    "  for paper_id, summary_embed in DICT_SUMMARY.items():\n",
    "    TaCC_embed = DICT_TaCC_EMBED[paper_id]\n",
    "    total_cosine_sum += cosine(summary_embed, TaCC_embed)\n",
    "    count += 1\n",
    "  return total_cosine_sum/float(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fos_lst = [\"artificial_intelligence\", \"machine_learning\", \"data_mining\"]\n",
    "\n",
    "summary_embed_path_list = [\"T5_Summary_w_title_w_topic_embeddings\", \"BART_Summary_w_title_w_topic_embeddings\", \n",
    "                           \"Pegasus_Summary_w_title_w_topic_embeddings\", \"ProphetNet_Summary_w_title_w_topic_embeddings\", \n",
    "                           \"MAGSumm_Summary_w_title_w_topic_embeddings\"]\n",
    "\n",
    "for fos in fos_lst:\n",
    "  print(f\"Results for {fos} \\n\")\n",
    "  for summary_embed_path in summary_embed_path_list:\n",
    "    avg_cosine = _compute_cosine_summary_w_TaCC(fos, summary_embed_path) * 100\n",
    "    print(f\"{summary_embed_path} cosine wrt TaCC: %.2f\" % avg_cosine)\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsE1GIKWaO2s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SPECTER_Semantic_Equivalence_Evaluation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
